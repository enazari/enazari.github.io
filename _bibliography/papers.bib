---
---

@inproceedings{pst,
  title={Using CGAN to Deal with Class Imbalance and Small Sample Size in Cybersecurity Problems},
  author={Nazari, Ehsan and Branco, Paula and Jourdan, Guy-Vincent},
  booktitle={2021 18th International Conference on Privacy, Security and Trust (PST)},
  pages={1--10},
  doi={10.1109/PST52912.2021.9647807},
  year={2021},
  month = {12 Dec},
  y = {1},
  organization={IEEE},
  preview={setting2.png},
  abstract = {Predictive modelling in cybersecurity domains usually involves dealing with complex settings. The class imbalance problem is a well-know challenge typically present in the cybersecurity domain. For instance, in a real-world intrusion detection scenario, the number of attacks is expected to be a a very small percentage of the normal cases. Moreover, in these applications, the number of available examples labelled is also small due to the complexity and cost of the labelling process: teams of domain experts need to be involved in the process which becomes expensive, time consuming and prone to errors. To address these problems is critical to the success of predictive modelling in cybersecurity applications.

  In this paper we tackle the class imbalance and small sample size through the use of a CGAN-based up-sampling procedure. We carry out an extensive set of experiments that show the positive impact of applying this solution to address the class imbalance and small sample size problems. A large data repository is built and freely provided to the research community containing 114 binary datasets based on real-world cybersecurity problems that are generated with diversified levels of imbalance and sample size. Our experiments show a clear advantage of using the CGAN-based up-sampling method specially for situations where the sample size is small and there is a large imbalance between the problem classes. In the most critical scenarios associated with extreme rarity and very small sample size, an impressive performance boost is achieved. We also explore the behaviour of this approach when the presence of these problems is less marked and we found that, while CGAN-based up-sampling is not able to further improve the minority class performance, it also has no negative impact. Thus, it is a safe to use solution, also in these scenarios. },
    url =    {https://ieeexplore.ieee.org/document/9647807},


}


@InProceedings{pmlr-v154-nazari21a,
  title =    {On Oversampling via Generative Adversarial Networks under Different Data Difficulty Factors},
  author =       {Nazari, Ehsan and Branco, Paula},
  booktitle =    {Proceedings of the Third International Workshop on Learning with Imbalanced Domains: Theory and Applications},
  pages =    {76--89},
  year =   {2021},
  editor =   {Moniz, Nuno and Branco, Paula and Torgo, Luis and Japkowicz, Nathalie and Woźniak, Michał and Wang, Shuo},
  volume =   {154},
    y = {1},
  series =   {Proceedings of Machine Learning Research},
  month =    {17 Sep},
  publisher =    {PMLR},
  pdf =    {https://proceedings.mlr.press/v154/nazari21a/nazari21a.pdf},
  url =    {https://proceedings.mlr.press/v154/nazari21a.html},
  abstract =   {Over the last two decades, several approaches have been proposed to tackle the class imbalance problem which is characterized by the inability of a learner to focus on a relevant but scarcely represented class. The generation of synthetic examples to oversample the training set and thus force the learner to focus on the important cases is one of such solutions. Recently, generative adversarial networks (GANs) started to be explored as an oversampling alternative due to their capability of generating samples from an implicit distribution. Still, data difficulty factors such as class overlap, data dimensionality or sample size, and were shown to also negatively impact the learners performance under an imbalance setting. The ability of GANs to deal with the imbalance problem and other data difficulty factors has not yet been assessed. The main goal of this paper is to understand how data difficulty factors impact the performance of GANs when they are used as an oversampling method. Namely, we study the performance of conditioned GANs (CGANs) in an image dataset with controlled levels of the following data difficulty factors: sample size, data dimensionality, class overlap and imbalance ratio. We show that CGANs are effective for tackling tasks with multiple data difficulty factors, exhibiting increased gains on the most difficult tasks.},
  preview={mnist.png}

}





@Article{math11040977,
AUTHOR = {Nazari, Ehsan and Branco, Paula and Jourdan, Guy-Vincent},
TITLE = {AutoGAN: An Automated Human-Out-of-the-Loop Approach for Training Generative Adversarial Networks},
JOURNAL = {Mathematics},
VOLUME = {11},
year = {2023},
number = {4},
ARTICLE-NUMBER = {977},
url = {https://www.mdpi.com/2227-7390/11/4/977},
ISSN = {2227-7390},
abstract = {Generative Adversarial Networks (GANs) have been used for many applications with overwhelming success. The training process of these models is complex, involving a zero-sum game between two neural networks trained in an adversarial manner. Thus, to use GANs, researchers and developers need to answer the question: &ldquo;Is the GAN sufficiently trained?&rdquo;. However, understanding when a GAN is well trained for a given problem is a challenging and laborious task that usually requires monitoring the training process and human intervention for assessing the quality of the GAN generated outcomes. Currently, there is no automatic mechanism for determining the required number of epochs that correspond to a well-trained GAN, allowing the training process to be safely stopped. In this paper, we propose AutoGAN, an algorithm that allows one to answer this question in a fully automatic manner with minimal human intervention, being applicable to different data modalities including imagery and tabular data. Through an extensive set of experiments, we show the clear advantage of our solution when compared against alternative methods, for a task where the GAN outputs are used as an oversampling method. Moreover, we show that AutoGAN not only determines a good stopping point for training the GAN, but it also allows one to run fewer training epochs to achieve a similar or better performance with the GAN outputs.},
doi = {10.3390/math11040977},
preview={mfid.png}
}